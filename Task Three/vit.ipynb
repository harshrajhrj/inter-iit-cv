{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059f7d8d",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "Step-by-step explanation of Vision Transformer Architecture\n",
    "1. Image to patches\n",
    "2. Patch EMbedding\n",
    "3. Add positional Embeddings\n",
    "4. Add [CLS] token\n",
    "5. Transformer Encoder Layers\n",
    "6. Classification Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dee1a7",
   "metadata": {},
   "source": [
    "##### 1. Image to Patches\n",
    "Segment image into patches/grids (fixed-size like 4x4 or 16x16). Because this process is equivalent to the process of tokenization in NLP models (segement a sequence of text into tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0129f",
   "metadata": {},
   "source": [
    "##### 2. Patch Embedding\n",
    "Flatten the patches into 1D vector (because transformer deals with 1D data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f8a49",
   "metadata": {},
   "source": [
    "##### 3. Add positional Embeddings\n",
    "We pass the vectors to the embedding layer to map these vectors/patches into high dimension dense vectors and then we add positional embeddings to preserve the order of the patches in order to encode positional information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29f99a",
   "metadata": {},
   "source": [
    "##### 4. Add [CLS] Token\n",
    "CLS Token is a special learnable token or vector that represents the whole image and this CLS Token will be used for classification. The output of this CLS token will be used later on for classification. [Patch token + Patch Embedding]. Special token to summarize image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5301e6",
   "metadata": {},
   "source": [
    "##### 5. Transformer Encoder Layers\n",
    "Consist of numerous blocks. Pass the output of [Patch token + Patch embedding] to the encoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95860b",
   "metadata": {},
   "source": [
    "##### 6. Classification Head\n",
    "Pass the output of transformer to the classification head to classify it into predefined classes. For example....CIFAR 10 (trained) will result output in either of 10 classes (one probability value of each class and highest probability is chosen using the arg max function to choose the predicted class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf35da",
   "metadata": {},
   "source": [
    "#### Comparative Study\n",
    "| NLP Transformer | Vision Transformer (VIT) | Explanation |\n",
    "| --- | --- | --- |\n",
    "| Tokens | Patches | Words/subwords in text -> Image split into small fixed-size patches |\n",
    "| Token IDs| Patch Indices | Each patch can be indexed like tokens |\n",
    "| Token Embedding | Patch Embedding | Converts token/patch index into dense vectors via learned projection |\n",
    "| Positional Embedding | Positional Embedding | Adds location information for sequence order/spatial structure |\n",
    "| [CLS] Token | [CLS] Token | Special token to summarize sequence/image -> used for classification |\n",
    "| Encoder Input Sequence | Embedded Patch Sequence | Input to Transformer: tokens + positional encoding OR patches + positional info |\n",
    "| Transformer Encoder Layers | Transformer Encoder Layers | Identical architecture for modeling dependencies |\n",
    "| Output Token Representation | [CLS] Token Representation | Used to generate final prediction (e.g., class label) |\n",
    "| Softmax Layer | Classification Head (MLP + Softmax) | Converts final output to class probabilites |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f8472",
   "metadata": {},
   "source": [
    "Step-by-Step ViT Pipeline Image to Patches\n",
    "- The input imgae (e.g., 224x224x3) is divided into fixed-size patches (e.g., 16x16)\n",
    "- This converts the image into a sequence of small flattened grids.\n",
    "- Patch Embedding\n",
    "- Each patch is flattened into a vector and passed through a **linear projection layer** to embed it into a fixed-length vector (like word embeddings in NLP).\n",
    "    - IN VIT, the embedding layer used is a linear layer and not as embedding layer as in NLP.\n",
    "- Add Positional Embeddings\n",
    "- Since Transformers have no built-in sense of order, positional embeddings are added to each patch embedding to presever spatial information.\n",
    "- Add [CLS] Token\n",
    "- A special learnable token [CLS] is prepended to the sequence. Its output after the Transformer will represent the whole image (used for classification).\n",
    "- Transformer Encoder Layers\n",
    "- The sequence (patch embeddings + CLS + positional encoding) is fed into standard Transformer encoder layers:\n",
    "- Multi-Head Self-Attention\n",
    "- Add & Norm\n",
    "- Feed Forward Network\n",
    "- Add & Norm\n",
    "- CLassification Head\n",
    "- The final hidden state of the [CLS] token is passed through a classification layer (typically an MLP) to produce the predicted image class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97781cd1",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db994f9",
   "metadata": {},
   "source": [
    "##### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969bb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random # generate random indexes to visualize some images randomly\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d4c583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.1+cu126'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040c4d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.1+cu126'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee32a06",
   "metadata": {},
   "source": [
    "##### Setup Device-Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035c7452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff3b61a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974034d9",
   "metadata": {},
   "source": [
    "##### Set the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bad6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51f963",
   "metadata": {},
   "source": [
    "##### Setting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e734d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "PATCH_SIZE = 4\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "DEPTH = 6\n",
    "MLP_DIM = 512\n",
    "DROP_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d193bd",
   "metadata": {},
   "source": [
    "##### Define Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b82cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "    # helps the model to converge faster and\n",
    "    # also it helps to make numerical computations stable\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fe4b9",
   "metadata": {},
   "source": [
    "##### Getting a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d8523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81473b98",
   "metadata": {},
   "source": [
    "##### Converting our datasets into dataloaders\n",
    "- Right now, our data is in the form of PyTorch Datasets.\n",
    "- DataLoader turns our data into batches or (mini-batches)\n",
    "    - It is more computationally efficient, as in, our computing hardware may not be able to look (store in memory) at 50000 images in one hit. So we break it into 128 images at a time. (batch size of 128).\n",
    "    - It gives our NN more chances to update its gradients per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ad047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
